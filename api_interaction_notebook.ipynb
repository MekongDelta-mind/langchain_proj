{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4005975-0a94-489c-ac0c-cf9729a9ee69",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# HuggingFaceHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8c0965b-2d89-480a-ac20-6b4411e87a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import streamlit as st\n",
    "from constants import hugging_face_access_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c74a83ac-3ca3-4021-b405-f38e1ef0e185",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import HuggingFaceHub, LLMChain, PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2debd03b-e115-440c-9a52-6c692d2436b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.16\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0164f60a-7a32-4018-9ba4-df3460aaa4f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/prabin_nayak/anaconda3/bin/python\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4f10f43-471d-4d34-82bf-1807c6524e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "!which langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f9c606b-56f5-4e9d-8fb5-2acbfc179d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing the constants\n",
      "initialed the constants\n"
     ]
    }
   ],
   "source": [
    "# initializing the reqiured constants used in the file\n",
    "print(f\"initializing the constants\")\n",
    "HUGGINGFACEHUB_API_TOKEN = hugging_face_access_key\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = HUGGINGFACEHUB_API_TOKEN\n",
    "print(f\"initialed the constants\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05faa53-76d0-4bd1-81e3-b416a51da25a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Selecting a model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bc64db-a96f-4eff-b362-b210930d7c1f",
   "metadata": {},
   "source": [
    "### Google models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7578b94e-f687-40cd-b5ef-d3f295d67da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selecting a model\n"
     ]
    }
   ],
   "source": [
    "# Selecting a model\n",
    "print(f\"selecting a model\")\n",
    "repo_id = \"google/flan-t5-xl\"  # See https://huggingface.co/models?pipeline_tag=text-generation&sort=downloads for some other options\n",
    "# repo_id = \"facebook/bart-base\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72ca07c-6170-4713-bc20-3fda92e11151",
   "metadata": {},
   "source": [
    "### StableLM: Stability AI Language Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cbfb418d-ef66-45c0-abce-f0143ef33a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selecting a model\n"
     ]
    }
   ],
   "source": [
    "# Selecting a model\n",
    "print(f\"selecting a model\")\n",
    "repo_id = \"CarperAI/stable-vicuna-13b-delta\"  # See https://huggingface.co/models?pipeline_tag=text-generation&sort=downloads for some other options\n",
    "# repo_id = \"facebook/bart-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3931add8-e317-4408-bc33-76290fa59e02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prabin_nayak/anaconda3/envs/llm_proj/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected model with id: CarperAI/stable-vicuna-13b-delta\n"
     ]
    }
   ],
   "source": [
    "llm = HuggingFaceHub(\n",
    "    repo_id=repo_id, model_kwargs={\"temperature\": 0.8, \"max_length\": 5000}\n",
    ")\n",
    "print(f\"selected model with id: {repo_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d40ba99-9f9e-4abc-98f2-d3e44d236604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "template initialized\n",
      "prompt initialized with template : \n",
      "Question: {question}\n",
      "\n",
      "Answer: Let's think step by step.\n",
      "template initialized\n"
     ]
    }
   ],
   "source": [
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "print(f\"template initialized\")\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "print(f\"prompt initialized with template : \\n{template}\")\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "print(f\"template initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4adae6ab-2600-4475-9df8-597127b9f7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Who won the FIFA World Cup in the year 1994? \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7ca74e-ad51-40d8-8288-b01e334318f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running the llm_chain\n"
     ]
    }
   ],
   "source": [
    "print(f\"running the llm_chain\")\n",
    "print(llm_chain.run(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e648205-b3b7-4591-ac4b-018aeb880094",
   "metadata": {},
   "source": [
    "# GPT4All "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed19cbf4-bd96-4585-930b-de383f959173",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gpt4all import GPT4All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1de5658d-64a6-4327-945a-087c9c3319b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# NEVER RUN IN LOCAL MACHINE\n",
    "#!wget https://huggingface.co/mrgaang/aira/resolve/main/gpt4all-converted.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27043535-d2af-4f2f-b12e-9fe86fc83f49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = \"gpt4all-converted.bin\"\n",
    "model_path = \"./models/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d6f12b4-33db-47a5-9ff8-8bcb8ce1cbdd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found model file at  ./models/gpt4all-converted.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from ./models/gpt4all-converted.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32001\n",
      "llama_model_load_internal: n_ctx      = 2048\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =  59.11 KB\n",
      "llama_model_load_internal: mem required  = 5809.33 MB (+ 1026.00 MB per state)\n",
      "llama_init_from_file: kv self size  = 1024.00 MB\n"
     ]
    }
   ],
   "source": [
    "gptj = GPT4All(model_name=model_name, model_path=model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23c27e6d-b975-4af0-b971-42f9c18ecdcb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "messages = [{\"role\": \"user\", \"content\": \"Name 3 colors\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d01cc224-5022-4b7f-b759-effd44db2a1d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction: \n",
      "            The prompt below is a question to answer, a task to complete, or a conversation \n",
      "            to respond to; decide which and write an appropriate response.\n",
      "            \n",
      "### Prompt: \n",
      "Name 3 colors\n",
      "### Response:\n",
      "  \n",
      "Red, blue and green\n",
      "CPU times: user 5 µs, sys: 0 ns, total: 5 µs\n",
      "Wall time: 10.5 µs\n"
     ]
    }
   ],
   "source": [
    "gptj.chat_completion(messages)\n",
    "%time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606cdf29-f63c-4180-b465-79d4e18b28b3",
   "metadata": {},
   "source": [
    "## Using langchain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aca0d51e-71ac-4f72-a781-238e4ef479c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain import LLMChain, PromptTemplate\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.llms import GPT4All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4c979b57-cc20-4240-811d-9836cdc48c62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ebde3573-99a5-459f-99c6-289f825acfab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Callbacks support token-wise streaming\n",
    "callbacks = [StreamingStdOutCallbackHandler()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f64343dd-8d60-4290-afb1-1595175576b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mGPT4All\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mlc_kwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcache\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseCallbackHandler\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseCallbackManager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcallback_manager\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseCallbackManager\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtags\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mbackend\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mn_ctx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mn_parts\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mseed\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mf16_kv\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mlogits_all\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mvocab_only\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0muse_mlock\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0membedding\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mn_threads\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mn_predict\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtemp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtop_p\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.95\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtop_k\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mecho\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mstop\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mrepeat_last_n\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mrepeat_penalty\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mn_batch\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mstreaming\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcontext_erase\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mallow_download\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mclient\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Wrapper around GPT4All language models.\n",
       "\n",
       "To use, you should have the ``gpt4all`` python package installed, the\n",
       "pre-trained model file, and the model's config information.\n",
       "\n",
       "Example:\n",
       "    .. code-block:: python\n",
       "\n",
       "        from langchain.llms import GPT4All\n",
       "        model = GPT4All(model=\"./models/gpt4all-model.bin\", n_ctx=512, n_threads=8)\n",
       "\n",
       "        # Simplest invocation\n",
       "        response = model(\"Once upon a time, \")\n",
       "\u001b[0;31mInit docstring:\u001b[0m\n",
       "Create a new model by parsing and validating input data from keyword arguments.\n",
       "\n",
       "Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
       "\u001b[0;31mFile:\u001b[0m           ~/.conda/envs/llm_proj/lib/python3.9/site-packages/langchain/llms/gpt4all.py\n",
       "\u001b[0;31mType:\u001b[0m           ModelMetaclass\n",
       "\u001b[0;31mSubclasses:\u001b[0m     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # # TO CHECK Class signture\n",
    "# ?GPT4All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "605a4621-459c-488c-b19f-a77159680c49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "local_path = model_path + model_name  # replace with your desired local file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7fc1bb18-ed0f-4542-a8c9-6fddd943e7f6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found model file at  ./models/gpt4all-converted.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from ./models/gpt4all-converted.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32001\n",
      "llama_model_load_internal: n_ctx      = 2048\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =  59.11 KB\n",
      "llama_model_load_internal: mem required  = 5809.33 MB (+ 1026.00 MB per state)\n",
      "llama_init_from_file: kv self size  = 1024.00 MB\n"
     ]
    }
   ],
   "source": [
    "# Verbose is required to pass to the callback manager\n",
    "llm = GPT4All(model=local_path, callbacks=callbacks, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f7854bfe-cb74-401c-b0b5-7a07f973f5ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TRY THIS OUT IFF THE ABOVE MODEL WORKS PROPERLY\n",
    "# # If you want to use a custom model add the backend parameter\n",
    "# # Check https://docs.gpt4all.io/gpt4all_python.html for supported backends\n",
    "# llm = GPT4All(model=local_path, backend=\"gptj\", callbacks=callbacks, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9f9b2859-482d-4054-8245-d52aee168d49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bdb7acb2-c91c-4682-a0ef-e9443244ad23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question = \"What NFL team won the Super Bowl in the year Justin Bieber was born?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9af9b0-eb6a-4e2f-8746-258f1b5d7f1f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Nebari resultsm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7390cd30-48ed-468b-bf47-074ef58e34de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " To answer this question, we first need to establish when and where a certain person (Justin) was born - which I will assume is January 1st of any given calendar year without further input or confirmation from the user asking for help here on Stack Exchange.\n",
      "Then, as Justin Bieber's birthday happened in 2093 at this moment to me; let us try and identify NFL team that won Super Bowl around his time span (1st January - 6th month), we can look back into the yearbook of every single season since a football match started. As Justin Bieber was born on one specific day in which there are not many seasons, it's easier to find out as I mentioned earlier; but NFL team that won Super Bowl during his birth is tougher because he wasn’t even alive until after the game finished! The most recent NFL championship season ended up in February 1976. So we can conclude Justin Bieber was not yet born when any of these games were played, as they took place too far back to be relevant for his birth year or current age at this time (in terms he's been alive)\n",
      " To answer this question, we first need to establish when and where a certain person (Justin) was born - which I will assume is January 1st of any given calendar year without further input or confirmation from the user asking for help here on Stack Exchange.\n",
      "Then, as Justin Bieber's birthday happened in 2093 at this moment to me; let us try and identify NFL team that won Super Bowl around his time span (1st January - 6th month), we can look back into the yearbook of every single season since a football match started. As Justin Bieber was born on one specific day in which there are not many seasons, it's easier to find out as I mentioned earlier; but NFL team that won Super Bowl during his birth is tougher because he wasn’t even alive until after the game finished! The most recent NFL championship season ended up in February 1976. So we can conclude Justin Bieber was not yet born when any of these games were played, as they took place too far back to be relevant for his birth year or current age at this time (in terms he's been alive)CPU times: user 5 µs, sys: 0 ns, total: 5 µs\n",
      "Wall time: 9.78 µs\n"
     ]
    }
   ],
   "source": [
    "llm_chain.run(question)\n",
    "%time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b775313-7691-4939-8f6e-c9c96353a524",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Local system results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9fea2b-5bc7-40f1-8241-01db6b517cce",
   "metadata": {},
   "source": [
    " The person asking this question is most likely a fan of pop music, as they mention their favorite singer (Justin Bieber) and ask about an event from his birthday month or day. They might be interested in the NFL too because it’s related to sports that are popular among people who love pop culture such as Justin's fans/followers. So we can deduce, they were born around January 2009 (Justin Bieber) and from a certain country like Canada or United States since Canadian artists have the tendency of making it big in America with their music style. Now to answer this question - The NFL team that won Super Bowl during Justin's birth year is Pittsburgh Steelers which they beat Arizona Cardinals on February 1, 2009 by a score of 31-17 at University Of Phoenix Stadium in Glendale (Arizona).\n",
    " The person asking this question is most likely a fan of pop music, as they mention their favorite singer (Justin Bieber) and ask about an event from his birthday month or day. They might be interested in the NFL too because it’s related to sports that are popular among people who love pop culture such as Justin's fans/followers. So we can deduce, they were born around January 2009 (Justin Bieber) and from a certain country like Canada or United States since Canadian artists have the tendency of making it big in America with their music style. Now to answer this question - The NFL team that won Super Bowl during Justin's birth year is Pittsburgh Steelers which they beat Arizona Cardinals on February 1, 2009 by a score of 31-17 at University Of Phoenix Stadium in Glendale (Arizona).\n",
    "\n",
    "\" The person asking this question is most likely a fan of pop music, as they mention their favorite singer (Justin Bieber) and ask about an event from his birthday month or day. They might be interested in the NFL too because it’s related to sports that are popular among people who love pop culture such as Justin's fans/followers. So we can deduce, they were born around January 2009 (Justin Bieber) and from a certain country like Canada or United States since Canadian artists have the tendency of making it big in America with their music style. Now to answer this question - The NFL team that won Super Bowl during Justin's birth year is Pittsburgh Steelers which they beat Arizona Cardinals on February 1, 2009 by a score of 31-17 at University Of Phoenix Stadium in Glendale (Arizona).\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a657ffeb-1056-44bc-9ca2-1ab4d0f1ed41",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Passing one repo and calculating the complexity score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccfecd5-d10d-4c2a-9c0b-3c48b4a2f934",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5639bea7-7c8e-456c-9c71-ebe70b8833dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1b92b92a-a495-4660-b58e-f7d58d8a5b65",
   "metadata": {},
   "source": [
    "## References for creating GPT4All app:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22c624e-12d1-4231-92d6-74374107bc41",
   "metadata": {},
   "source": [
    "* https://www.youtube.com/watch?v=4p1Fojur8Zw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce842f4-8d9b-4a7c-8993-1ecc104d307d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_proj",
   "language": "python",
   "name": "llm_proj"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
